# -*- coding: utf-8 -*-
"""744 A100 Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DIt8rqXILy8OncmVDeS5cWO3PekXn60f
"""

# ────────────────────────────────────────────────────────────────────────────────
# 1. Install Required Packages (if not already installed)
# ────────────────────────────────────────────────────────────────────────────────
import subprocess
import sys
import os

def _pip_install_if_missing(package_names):
    for pkg in package_names:
        try:
            __import__(pkg)
        except ImportError:
            subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

_pip_install_if_missing([
    "torch",
    "transformers",
    "datasets",
    "evaluate",
    "rouge_score",
    "absl",
    "nltk",
    "sacrebleu",
    "tqdm",
    "sentencepiece"
])

# ────────────────────────────────────────────────────────────────────────────────
# 2. Imports
# ────────────────────────────────────────────────────────────────────────────────
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    get_scheduler
)
from datasets import load_dataset
import evaluate
import rouge_score
import sacrebleu
import nltk
from tqdm import tqdm

nltk.download("punkt", quiet=True)
os.environ["WANDB_DISABLED"] = "true"

# ────────────────────────────────────────────────────────────────────────────────
# 3. Define Variables (Simulating argparse)
# ────────────────────────────────────────────────────────────────────────────────
eval_only = False  # Set to True for evaluation only
checkpoint_dir = "./outputs_gen"  # Path to save/load model

# ────────────────────────────────────────────────────────────────────────────────
# 4. Define the Main Function
# ────────────────────────────────────────────────────────────────────────────────
def save_trained_model(model, checkpoint_dir):
    model.save_pretrained(checkpoint_dir)
    print(f"MODEL SUCCESSFULLY SAVED TO {checkpoint_dir}")

def main(eval_only, checkpoint_dir):
    assert torch.cuda.is_available(), "CUDA not available"
    device = torch.device("cuda", 0)
    print(f"Using GPU: {torch.cuda.get_device_name(device)}")

    # Load & preprocess dataset
    dataset_full = load_dataset("ServiceNow-AI/M2Lingual", "full_data")
    subset = dataset_full["train"]
    subset = subset.filter(lambda x: x.get("output_assistant_reply") not in [None, ""])
    split = subset.train_test_split(test_size=0.1, seed=42)
    train_ds, eval_ds = split["train"], split["test"]

    model_name = "google/mt5-small"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def preprocess(ex):
        turns = ex.get("conversation", [])
        language = ex.get("language", "unknown")
        task = ex.get("task", "general")
        evolved_user_prompt = ex.get("evolved_user_prompt", "")
        evolved_multiturn_prompt = ex.get("evolved_multiturn_prompt", "")
        target_text = ex.get("output_assistant_reply", "")

        if not isinstance(turns, list) or not isinstance(target_text, str) or not target_text.strip():
            return None

        input_turns = []
        if isinstance(evolved_multiturn_prompt, str) and evolved_multiturn_prompt.strip():
            input_turns.append(f"User: {evolved_multiturn_prompt}")
        elif isinstance(evolved_user_prompt, str) and evolved_user_prompt.strip():
            input_turns.append(f"User: {evolved_user_prompt}")
        else:
            for t in turns[:-1]:
                if not isinstance(t, dict): continue
                role = t.get("role", "").capitalize()
                content = t.get("content", "")
                if not isinstance(content, str) or not content.strip(): continue
                input_turns.append(f"{role}: {content}")

        input_text = f"[LANGUAGE: {language}] [TASK: {task}]\n" + "\n".join(input_turns)

        try:
            inputs = tokenizer(input_text, padding="max_length", truncation=True, max_length=512)
            targets = tokenizer(target_text, padding="max_length", truncation=True, max_length=128)
            inputs["labels"] = targets["input_ids"]
            return inputs
        except Exception as e:
            print(f"Tokenization failed: {e}")
            return None

    train_tok = train_ds.map(preprocess, remove_columns=train_ds.column_names)
    eval_tok = eval_ds.map(preprocess, remove_columns=eval_ds.column_names)
    cols = ["input_ids", "attention_mask", "labels"]
    train_tok.set_format(type="torch", columns=cols)
    eval_tok.set_format(type="torch", columns=cols)

    train_loader = DataLoader(train_tok, batch_size=4, shuffle=True) if not eval_only else None
    eval_loader = DataLoader(eval_tok, batch_size=4)

    src = checkpoint_dir if eval_only else model_name
    model = AutoModelForSeq2SeqLM.from_pretrained(src).to(device)

    # Training loop
    if not eval_only:
        optimizer = AdamW(model.parameters(), lr=5e-5)
        num_steps = len(train_loader) * 3
        lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_steps)
        model.train()
        for epoch in range(3):
            print(f"\nStarting epoch {epoch}")
            # Use tqdm for the progress bar in the training loop
            with tqdm(train_loader, desc=f"Epoch {epoch}", unit="batch", dynamic_ncols=True) as pbar:
                for step, batch in enumerate(pbar):
                    batch = {k: v.to(device) for k, v in batch.items()}
                    outputs = model(**batch)
                    loss = outputs.loss
                    loss.backward()
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()

                    # Update the progress bar with the loss information
                    pbar.set_postfix(loss=loss.item())
            print(f"Finished epoch {epoch}; last loss = {loss.item():.4f}")
        save_trained_model(model, checkpoint_dir)

    # Evaluation
    model.eval()
    rouge = evaluate.load("rouge")
    bleu = evaluate.load("bleu")
    all_preds, all_refs = [], []
    for batch in tqdm(eval_loader, desc="Generating"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        gen_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,
            max_length=128,
            num_beams=4,
            early_stopping=True
        )
        preds = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
        refs = tokenizer.batch_decode(batch["labels"], skip_special_tokens=True)
        for p, r in zip(preds, refs):
            if r.strip():
                all_preds.append(p)
                all_refs.append(r)

    if not all_refs:
        print("No non-empty references found; skipping BLEU/ROUGE.")
    else:
        bleu_scores = bleu.compute(predictions=all_preds, references=all_refs)
        rouge_scores = rouge.compute(predictions=all_preds, references=all_refs, use_stemmer=True)
        print("\n=== Generation Metrics ===")
        print(f"BLEU: {bleu_scores['bleu']:.4f}")
        for name, score in rouge_scores.items():
            print(f"ROUGE-{name.upper()}: {score:.4f}")
    model.save_pretrained("./outputs_gen/")

# ────────────────────────────────────────────────────────────────────────────────
# 5. Run the Main Function
# ────────────────────────────────────────────────────────────────────────────────
main(eval_only, checkpoint_dir)

import zipfile
import os

def zip_folder(folder_path, zip_path):
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                # Write file with relative path to preserve folder structure
                arcname = os.path.relpath(file_path, start=folder_path)
                zipf.write(file_path, arcname)

# Example usage
zip_folder('./outputs_gen', './outputs_gen.zip')
